{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Partition Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>partition</b> of a set is a grouping of the set's elements into non-empty subsets, in such a way that every element is included in one and only one of the subsets.\n",
    "$$C_1,C_2,...,C_K  \\text{ is a partition of } \\left \\{1,2,...,n\\right \\}$$ \n",
    "$$\\text{iff} \\qquad C_1 \\cup C_2 \\cup  ... \\cup C_K  = \\left \\{1,2,...,n\\right \\}$$\n",
    "$$\\text{and} \\quad C_i \\cap C_j = \\varnothing \\quad \\forall i\\neq j i,\\quad j \\in \\left \\{1,2,...,n\\right \\}$$\n",
    "$$\\therefore \\text{a partition can be} \\left \\{1\\right \\}, \\left \\{3\\right \\},\\left \\{2\\right \\}; \\quad   \\left \\{1\\right \\}, \\left \\{3, 2\\right \\}; \\quad or  \\left \\{3, 1, 2\\right \\} \\text{ for set } \\left \\{1, 2, 3\\right \\}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While classification takes the training set $S_n = \\left \\{ x^{(i)}, y^{(i)} | i = 1,2,...,n \\right \\}$ and a number of classes K as inputs, \n",
    "clustering's inputs are the training set $S_n = \\left \\{ x^{(i)} | i = 1,2,...,n \\right \\}$ and a number of clusters K.\n",
    "\n",
    "The outputs of clustering are a partition of indices $\\left \\{ 1,2,...,n \\right \\}$ into K sets, $C_1, C_2, ..., C_K$ and \"representatives\" in each of the K partition sets, given as $z_1, z_2, ..., z_K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good method for deciding which clustering output is more preferable is to define a measure of homogeneity inside cluster assignments and compare the measure of each scenario.\n",
    "$$Cost(C_1, C_2, ... , C_K) = \\sum_{j=1}^{K}Cost(C_j)$$\n",
    "\n",
    "Ways to measure \"how homogeneous\" the assigned data are inside the  ùëó th cluster $C_j ,\\quad Cost(C_j)$ :\n",
    "\n",
    "1. The average distance between points inside a cluster\n",
    "\n",
    "2. The sum of distance between the representative and all points inside a cluster\n",
    "\n",
    "Similarity Measure:\n",
    "\n",
    "- Euclidian distance: if the length of a vector needs to take into consideration.\n",
    "- Cosine distance\n",
    "\n",
    "3. The diameter of a cluster: \n",
    "a measure of how far the two farthest points in a cluster are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùë•1=(‚àí1,2),  ùë•2=(‚àí2,1),  ùë•3=(‚àí1,0),  ùëß1=(‚àí1,1) \t \t \n",
    "ùë•4=(2,1), ùë•5=(3,2), ùëß2=(2,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost of a clustering output is given by the sum of the squared euclidean distance of all points in a cluster with the representative for each of its cluster.\n",
    "$$Cost(C_1, C_2, ... , C_K) = \\sum_{j=1}^{K}Cost(C_j)=\\sum_{j=1}^{K}\\sum_{i\\in C_j}\\left \\| z_j-x_i \\right \\|^2=5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">K-Means Algorithm </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly select $z_1, ... , z_K$\n",
    "\n",
    "2. Iterate\n",
    "    1. Given  $z_1, ... , z_K$ , assign each data point  ùë•(ùëñ)  to the closest  ùëßùëó , so that $$Cost(z_1,  ... , z_K) =\\sum_{i=1}^{n}\\underset{j=1,...,K}{min}\\left \\| z_j-x^{(i)} \\right \\|^2$$\n",
    "<img width=\"450\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec12_kmeans1.svg\">\n",
    "    2. Given  $C_1, ... , C_K$  find the best representatives  $z_1, ... , z_K$ , i.e. find  $z_1, ... , z_K$  such that\n",
    "$$z_j =argmin_z\\sum_{i\\in C_j}\\left \\| z-x^{(i)} \\right \\|^2$$ \n",
    "\n",
    "<img width=\"450\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec12_kmeans2.svg\">\n",
    "\n",
    "$$\\nabla_{z_j} \\left ( \\sum_{i\\in \\mathbb{C}_j}\\left \\| z_j-x^{(i)} \\right \\|^2 \\right )=\\sum_{i\\in \\mathbb{C}_j}-2(z_j-x^{(i)})=0 \\quad \\Rightarrow \\quad z_j=\\frac{\\sum_{i\\in \\mathbb{C}_j}x^{(i)}}{\\left | \\mathbb{C}_j \\right |}$$\n",
    "\n",
    "   where $\\left | \\mathbb{C}_j \\right |$ is the number of data points in the ùëó th cluster\n",
    "  - The value of  $z_j$  is only affected by points  $\\left \\{ x_i:i \\in \\mathbb{C}_j \\right \\}$\n",
    "  - The obtained  $z_j$  is the centroid (the center of mass) of the  ùëó th cluster\n",
    "\n",
    "\n",
    "3. Replace the initial $z_1, ... , z_K$ by the new representztives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of K-means\n",
    "1. Manual choice of  ùêæ\n",
    "\n",
    "2. Not robust to outliers\n",
    "- Centroids can be dragged around by outliers or outliers might get their own cluster.\n",
    "3. Does not scale well with increasing number of dimensions\n",
    "- A distance-based similarity measure converges to a constant value between any given examples.\n",
    "4. Dependent on initial values\n",
    "- While Steps 2.1 and 2.2 of the algorithm always decrease the cost or keep it the same at least, the output of the algorithm largely depends upon the initialization in Step 1. <br>Thus, in practice it is wise to make sure that  $z_1,...,z_K$  are initialized so that they are well spread out. Another alternative is to try multiple initializations and choose the clustering output that appears the most commonly.\n",
    "5. Difficult in clustering data of varying sizes and density\n",
    "- Suppose we have a dataset drawn from 2 different Gaussian distribution  $N(\\mu_1,\\sigma_1^2) ,\\quad  N(\\mu_2,\\sigma_2^2)\\quad$ where $\\quad \\mu_1 \\neq \\mu_2 \\quad \\sigma_1^2 \\ll \\sigma_2^2 $ . <br>The boundary between the 2 optimal clusters is closer to one centroid then the other. Since the 2-means algorithm will always have an equidistant split between the two centroids, this behavior cannot be reproduced and thus k-means clustering will erroneoously assign more points to the cluster with a smaller variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">K-Medoids Algorithm </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly select $\\left \\{ z_1, ... , z_K \\right \\}\\subseteq\\left \\{ x_1, ... , x_n \\right \\}$\n",
    "\n",
    "2. Iterate\n",
    "    1. Given  $z_1, ... , z_K$ , assign each  ùë•(ùëñ)  to the closest  ùëßùëó , so that $$Cost(z_1,  ... , z_K) =\\sum_{i=1}^{n}\\underset{j=1,...,K}{min}dist( z_j,x^{(i)} )$$\n",
    "    \n",
    "    2. Given  $C_1, ... , C_K$  find the best representatives  $z_1, ... , z_K$ such that\n",
    "$$z_j =argmin_z\\sum_{x^{(i)} \\in \\mathbb{C}_j}dist( z_j,x^{(i)} )$$ \n",
    "\n",
    "3. Replace the initial $z_1, ... , z_K$ by the new representztives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Generative vs Discriminative models </span>\n",
    "- Generative models model the probability distribution of each class\n",
    "- Discriminative models learn the decision boundary between the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models\n",
    "1. Multinomials\n",
    "$$\\theta_w\\geqslant 0 \\quad \\text{and} \\quad \\sum_{w \\in \\mathbb{W}}\n",
    "\\theta_w=1 \\quad \\text{where} \\quad \\theta_w = P(w|\\theta)$$\n",
    "$$P(D|\\theta)=\\prod_{i=1}\\theta_{w_i}=\\prod_{w \\in \\mathbb{W}} \\theta_w^{count(w)}$$\n",
    "\n",
    "2. Gaussians\n",
    "\n",
    "How to estimate the model (find the right type of probability distribution to describe each class)?\n",
    "How to do prediction after estimation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
