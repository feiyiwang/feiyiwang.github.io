{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from scipy.ndimage import convolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <span style=\"color:blue\"><b>neural network unit</b></span> is a primitive neural network that consists of only the ‚Äúinput layer\", and an output layer with only one output. It is represented pictorially as follows:<br>\n",
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_neuralnetunit.svg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Feed-forward Neural Networks (FFNNs)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>deep (feedforward) neural network</b> refers to a neural network that contains not only the input and output layers, but also hidden layers in between. For example, below is a deep feedfoward neural network of 2 hidden layers, with each hidden layer consisting of 5 units:<br>\n",
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_deepneuralnet.svg\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Deep Learning\n",
    "1. lots of data - many significant problems can only be solved at scale\n",
    "2. computational resources (esp. GPUs) - platforms/systems that support running deep (machine) learning algorithms at scale\n",
    "3. large models are easier to train - large models can be successfully estimated with simple gradient based learning algorithms\n",
    "4. flexible neural ‚Äúlego pieces‚Äù - common representations, diversity of architectural choices\n",
    "\n",
    "### Advantages\n",
    "1. One of the main advantages of deep neural networks is that in many cases, they can learn to extract very complex and sophisticated features from just the raw features presented to them as their input. For instance, in the context of image recognition, neural networks can extract the features that differentiate a cat from a dog based only on the raw pixel data presented to them from images.\n",
    "2. The initial few layers of a neural networks typically capture the simpler and smaller features whereas the later layers use information from these low-level features to identify more complex and sophisticated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 Representation Power of Neural Networks\n",
    "The logic NAND function is defined as $y=NOT(x_1 $ AND $ x_2)$ where  ùë•1  and  ùë•2‚àà{0,1}  are binary inputs (and  1  denotes  True  and  0  denotes  False ).<br>\n",
    "If the activation function is the step function $U(z)=\\begin{cases} 0 & \\text{ if } z \\leqslant 0 \\\\ 1 & \\text{ if } z >0 \\end{cases}$, write a possible combination of $w_0, w_1, w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_logicNAND.svg\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b><br>\n",
    "$\\because NAND(x_1,x_2)=\\begin{cases} 0 & \\text{ if } (x_1,x_2)=(1,1) \\\\ 1 & \\text{ otherwise }  \\end{cases}$ and $U(z)=\\begin{cases} 0 & \\text{ if } z \\leqslant 0 \\\\ 1 & \\text{ if } z >0 \\end{cases}$.<br>\n",
    "$\\therefore$ when $x_1=x_2=1, w_1x_1+w_2x_2+w_0=w_1+w_2+w_0\\leqslant0$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;when $x_1=x_2=0, w_1x_1+w_2x_2+w_0=w_0>0$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;when $x_1=1, x_2=0, w_1x_1+w_2x_2+w_0=w1+w_0>0$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;when $x_1=0, x_2=1, w_1x_1+w_2x_2+w_0=w_2+w_0>0$<br>\n",
    "So, one example can be $w_0=6, w_1=-5, w_2=-5$\n",
    "<br><br>\n",
    "NAND function is known as a universal logic function, which can be used to implement any boolean functions, including also  XOR , without the use of any other type of function (except for the identity and zero function).<br>\n",
    "Use the NAND function only as the basic neural network unit and De Morgan's law in boolean algebra. <br><br>\n",
    "<b>AND  function:</b>\n",
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_logicAND.svg\" width=\"250\"><br>\n",
    "Here, each pair of edges of the same color along with the nodes they are connected to form a neural network unit that represents the NAND function. (They do not represent values of inputs or outputs). In the example above,  ùë•1  and  ùë•2  are inputs to two NAND units, and are connected to output of respective units by the blue and orange arrows.<br>\n",
    "<b>NOT function:</b>\n",
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_logicNOT.svg\" width=\"250\"><br>NOT(ùë•1 AND ùë•1)=NOT ùë•1, <br><br>\n",
    "<b>OR function:</b>\n",
    "<img style=\"float: center;\" src=\"https://feiyiwang.github.io/notebook/jupyter/img/images_lec8_logicOR.svg\" width=\"250\"><br>NOT( NOT (ùë•1) AND NOT(ùë•2))=NOT( NOT(ùë•1 OR ùë•2))=ùë•1 OR ùë•2. <Br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer representation\n",
    "<img src=\"img/8.1.png\" width=\"500\"/><img src=\"img/8.2.png\" width=\"500\"/>\n",
    "<img src=\"img/8.3.png\" width=\"500\"/>\n",
    "### Does orientation matter?\n",
    "<img src=\"img/8.4.png\" width=\"500\"/><img src=\"img/8.5.png\" width=\"500\"/>\n",
    "### Random hidden units\n",
    "<img src=\"img/8.6.png\" width=\"500\"/><br><img src=\"img/8.7.png\" width=\"500\"/>\n",
    "<img src=\"img/8.8.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Units in neural networks are linear classifiers, just with different output non-linearity\n",
    "2. The units in feed-forward neural networks are arranged in layers (input, hidden,..., output)\n",
    "3. By learning the parameters associated with the hidden layer units, we learn how to represent examples (as hidden layer activations)\n",
    "4. The representations in neural networks are learned directly to facilitate the end-to-end task\n",
    "5. A simple classifier (output unit) suffices to solve complex classification tasks if it operates on the hidden layer representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Back-propagation Algorithm</span>\n",
    "\n",
    "<img src=\"img/9.1.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Update: <br>\n",
    "$$w_1 \\leftarrow w_1 - \\eta\\cdot\\nabla_{w_1}Loss(y, f_L)$$\n",
    "\n",
    "Recursive Expression: <br>\n",
    "$$\\frac{\\partial Loss}{\\partial w_1}=\\frac{\\partial z_1}{\\partial w_1}\\frac{\\partial Loss}{\\partial z_1}$$<br>\n",
    "$$\\because z_1=w_1x\\quad\\rightarrow \\quad\\frac{\\partial z_1}{\\partial w_1}=\\frac{\\partial (w_1\\cdot x)}{\\partial w_1}=x$$<br>\n",
    "$$\\therefore \\frac{\\partial Loss}{\\partial w_1} = x \\delta_1 \\qquad(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain rule:<br>\n",
    "$$\\delta_1=\\frac{\\partial f_1}{\\partial z_1}\\cdot\\frac{\\partial z_2}{\\partial f_1}\\cdot\\frac{\\partial Loss}{\\partial z_2}$$\n",
    "$$\\because f_1=tanh(z_1) \\quad\\rightarrow\\quad\\frac{\\partial f_1}{\\partial z_1}=(1-f_1^2)$$\n",
    "$$\\textrm{and} z_2=w_2 \\cdot f_1\\quad\\rightarrow\\quad\\frac{\\partial f_2}{\\partial z_2}=w_2$$\n",
    "$$\\therefore \\delta_1=(1-f_1^2)\\cdot w_2 \\cdot\\frac{\\partial Loss}{\\partial z_2}=(1-f_1^2)\\cdot w_2 \\cdot \\delta_2\\qquad(2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Expression of the Gradient:<br>\n",
    "$$\\because \\textrm{(1) & (2)} \\quad \\rightarrow \\quad \\delta_{L-1}=(1-f_{L-1}^2)\\cdot w_L \\cdot \\delta_L$$\n",
    "$$\\therefore \\delta_L = \\frac{\\partial Loss}{\\partial z_L}=\\frac{\\partial Loss}{\\partial f_L} \\cdot \\frac{\\partial f_L}{\\partial z_L}=\\frac{\\partial (f_L-y)^2}{\\partial f_L}\\frac{\\partial f_L}{\\partial z_L}$$\n",
    "$$=2(f_L-y)\\frac{\\partial f_L}{\\partial z_L}=2(f_L-y)(1-f_L^2)$$\n",
    "$$\\therefore \\frac{\\partial Loss}{\\partial w_1}=x \\cdot \\delta_1=x \\cdot (1-f_1^2) \\cdot w_2 \\cdot \\delta_2 $$\n",
    "$$=x \\cdot (1-f_1^2) \\cdot w_2 \\cdot (1-f_2^2)\\cdot w_3 \\cdot\\delta_3$$ \n",
    "$$= ...$$\n",
    "$$=x \\cdot (1-f_1^2) \\cdot (1-f_2^2)\\cdot...\\cdot (1-f_L^2)\\cdot w_2 \\cdot w_3 \\cdot ...\\cdot(2(f_L-y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "1. Train 2 hidden units (Randomly initialized weights + zero offset)<br>\n",
    "<img src=\"img/9.2.png\" width=\"500\"/><br>\n",
    "After ~10 passes through the data<br>\n",
    "<img src=\"img/9.3.png\" width=\"500\"/>\n",
    "2. Train 10 hidden units (Randomly initialized weights + zero offset)<br>\n",
    "<img src=\"img/9.4.png\" width=\"500\"/>\n",
    "3. Cannot get solved via 2 hidden units<br>\n",
    "<img src=\"img/9.5.png\" width=\"250\"/>\n",
    "<img src=\"img/9.6.png\" width=\"250\"/>\n",
    "<img src=\"img/9.7.png\" width=\"250\"/>\n",
    "4. ReLU units\n",
    "- Many recent architectures use ReLU units (cheap to evaluate, sparsity)\n",
    "- Easier to learn as large models<br>\n",
    "<img src=\"img/9.8.png\" width=\"250\"/>\n",
    "<img src=\"img/9.9.png\" width=\"250\"/>\n",
    "<img src=\"img/9.10.png\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues: Exploding Gradient\n",
    "\n",
    "<b>What Are Exploding Gradients?</b>\n",
    "An error gradient is the direction and magnitude calculated during the training of a neural network that is used to update the network weights in the right direction and by the right amount.\n",
    "\n",
    "<b>What Is the Problem with Exploding Gradients?</b>\n",
    "In deep multilayer Perceptron networks, exploding gradients can result in an unstable network that at best cannot learn from the training data and at worst results in NaN weight values that can no longer be updated.\n",
    "\n",
    "<b>How do You Know if You Have Exploding Gradients?</b>\n",
    "Signs to pay attention:\n",
    "1. The model is unable to get traction on your training data (e.g. poor loss).\n",
    "2. The model is unstable, resulting in large changes in loss from update to update.\n",
    "3. The model loss goes to NaN during training.\n",
    "\n",
    "Signs to Confirm:\n",
    "1. The model weights quickly become very large during training.\n",
    "2. The model weights go to NaN values during training.\n",
    "3. The error gradient values are consistently above 1.0 for each node and layer during training.\n",
    "\n",
    "<b>How to Fix Exploding Gradients?</b>\n",
    "1. Re-Design the Network Model\n",
    "- exploding gradients may be addressed by redesigning the network to have fewer layers.\n",
    "\n",
    "In RNNs, gradient exploding can occur given the inherent instability in the training of this type of network, e.g. via Backpropagation through time that essentially transforms the recurrent network into a deep multilayer Perceptron neural network.\n",
    "\n",
    "2. Use Long Short-Term Memory Networks\n",
    "- perhaps because of the gated-type neuron structures.\n",
    "\n",
    "Exploding gradients can still occur in very deep Multilayer Perceptron networks with a large batch size and LSTMs with very long input sequence lengths.\n",
    "\n",
    "3. Use Gradient Clipping\n",
    "- check for and limit the size of gradients during the training of your network.\n",
    "\n",
    "4. Use Weight Regularization \n",
    "- check the size of network weights and apply a penalty to the networks loss function for large weight values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues: Vanishing Gradient\n",
    "\n",
    "<b>What Are Vanishing Gradients?</b>\n",
    "Back-propagation i.e moving backward in the Network and calculating gradients of loss(Error) with respect to the weights , the gradients tends to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly as compared to the neurons in the later layers in the Hierarchy. The Earlier layers in the network are slowest to train.\n",
    "\n",
    "<b>What Is the Problem with Vanishing Gradients?</b>\n",
    "The Training process takes too long and the Prediction Accuracy of the Model will decrease.\n",
    "\n",
    "<b>How to Fix Exploding Gradients?</b>\n",
    "1. In deep models, use simple functions such as ReLU\n",
    "- avoid using Sigmoid and Tanh as Activation functions which causes vanishing Gradient Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Neural networks can be learned with SGD similarly to linear classifiers\n",
    "2. The derivatives necessary for SGD can be evaluated effectively via back-propagation\n",
    "3. Multi-layer neural network models are complicated, so we are no longer guaranteed to reach global (only local) optimum with SGD\n",
    "4. Larger models tend to be easier to learn because their units only need to be adjusted so that they are, collectively, sufficient to solve the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Recurrent Neural Networks (RNNs)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward networks VS RNNs:\n",
    "When it comes to temporal sequences, RNNs in general can automatically address some issues that need to be engineered with feed-forward networks. \n",
    "\n",
    "E.g. How many time steps back should we look at in the feature vector?<br>\n",
    "How to retain important items mentioned far back?\n",
    "\n",
    "RNN can learn the data/history and encode it into a feature vector, unlike feed-forward networks.\n",
    "\n",
    "3 differences in architecture between encoder (unfolded RNN) and a standart feed-forward:\n",
    "- input is received at each layer (per word), not just at the beginning as in a typical feed-forward network\n",
    "- the number of layers varies, and depends on the length of the sentence\n",
    "- parameters of each layer (representing an application of an RNN) are shared (same RNN at each step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Encoding</span>\n",
    "- e.g., mapping a sequence to a vector\n",
    "\n",
    "Easy to introduce adjustable ‚Äúlego pieces‚Äù and optimize them for end-to-end performance\n",
    "<img src=\"img/10.1.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will expect the states that contain information about the phrase \"Efforts and courage\" at time steps 3 and onward to contain information about the first three words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate\n",
    "\n",
    "<img src=\"img/10.2.png\" width=\"450\"/>\n",
    "\n",
    "Here, gate vector $g_t$ has the same dimension as $S_t$, which determines \"how much information to overwrite in the next state.\" The sign  ‚®Ä  denotes element-wise multiplication.\n",
    "\n",
    "$\\therefore$ If the  ùëñ th element of  $g_{t}$  is 0, the  ùëñ th element of  $S_t$  and that of  $S_{t-1}$  are equal; \n",
    "\n",
    "$\\therefore$ If  $g_{t}$  is a vector whose elements are all 0,  $S_t$  and  $S_{t-1}$  are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1, -1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "w_fh, w_fx, b_f, w_ch = 0, 0, -100, -100\n",
    "w_ih, w_ix, b_i, w_cx = 0, 100, 100, 50\n",
    "w_oh, w_ox, b_o, b_c = 0, 100, 0, 0\n",
    "h_t, c_t, h = 0, 0, []\n",
    "\n",
    "def calc_func(wh, h, wx, x, b, func):\n",
    "    z = wh*h + wx*x + b\n",
    "    if func == 'sigmoid':\n",
    "        return 1 if z >= 1 else 0 if z <= -1 else 1/(1 + np.exp(-z))\n",
    "    elif func == 'tanh':\n",
    "        return 1 if z >= 1 else -1 if z <= -1 else np.tanh(z)\n",
    "    \n",
    "for x_t in [0, 0, 1, 1, 1, 0]: # For the LSTM unit\n",
    "    f_t = calc_func(w_fh, h_t, w_fx, x_t, b_f, 'sigmoid')\n",
    "    i_t = calc_func(w_ih, h_t, w_ix, x_t, b_i, 'sigmoid')\n",
    "    o_t = calc_func(w_oh, h_t, w_ox, x_t, b_o, 'sigmoid')\n",
    "    c_t = np.multiply(f_t, c_t) + np.multiply(i_t, calc_func(w_ch, h_t, w_cx, x_t, b_c, 'tanh'))\n",
    "    h_t = np.multiply(o_t, calc_func(0,0,0,0,c_t,'tanh'))\n",
    "    if h_t == 0.5 or h_t == -0.5:\n",
    "        h_t = 0\n",
    "    h.append(h_t)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-short Term Memory (LSTM)\n",
    "The diagram below shows a single LSTM unit that consists of Input, Output, and Forget gates.\n",
    "\n",
    "$f_t$ is <b>forget gate</b>; $i_t$ is <b>input gate</b>; $o_t$ is <b>output gate</b>; $c_t$ is <b>memory cell</b>; $h_t$ is <b>visible state</b>; \n",
    "\n",
    "<img src=\"img/images_hw4_p2.png\" width=\"450\"/><br>\n",
    "The behavior of such a unit as a recurrent neural network is specified by a set of update equations. These equations define how the gates, ‚Äúmemory cell\"  $c_t$  and the ‚Äúvisible state\"  $h_t$  are updated in response to input  $x_t$  and previous states  $c_{t-1}$ ,  $h_{t-1}$ . For the LSTM unit,<br>\n",
    "<img src=\"img/images_hw4_p3.png\" width=\"400\"/><br>\n",
    "where symbol  ‚äô  stands for element-wise multiplication. The adjustable parameters in this unit are matrices  $W^{f,h}$ ,  $W^{f,x}$ ,  $W^{i,h}$ ,  $W^{f,x}$ ,  $W^{o,h}$ ,  $W^{o,x}$ ,  $W^{c,h}$ ,  $W^{c,x}$ , as well as the offset parameter vectors  $b_f$ ,  $b_i$ ,  $b_o$ , and  $b_c$ . By changing these parameters, we change how the unit evolves as a function of inputs  $x_t$ .\n",
    "\n",
    "To keep things simple, in this problem we assume that  $x_t$ ,  $c_t$ , and  $h_t$  are all scalars. Concretely, suppose that the parameters are given by<br>\n",
    "<img src=\"img/images_hw4_p4.png\" width=\"400\"/><br>\n",
    "We run this unit with initial conditions  $h_{-1}=0$  and  $c_{-1}=0$ , and in response to the following input sequence: [0, 0, 1, 1, 1, 0] (For example,  $x_0=0$ ,  $x_1=0$ ,  $x_2=1$ , and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Decoding</span> \n",
    "\n",
    "<img src=\"img/12.22.png\" width=\"400\"/>\n",
    "\n",
    "### Markov Models\n",
    "Next word in a sentence depends on previous symbols already written. Say, use previous words to predict \"$bumfuzzled$\"\n",
    "$$The \\quad lecture \\quad leaves \\quad me \\quad bumfuzzled$$\n",
    "\n",
    "Let $w \\in V$ denote the set of possible words/symbols that includes unknown words (UNK), beginning (beg) and end\n",
    "$$<beg> \\quad The \\quad lecture \\quad leaves \\quad me \\quad UNK \\quad <end>$$\n",
    "$$w0 \\qquad w1 \\qquad w2 \\qquad w3 \\qquad w4 \\qquad w5 \\qquad w6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple first order Markov model\n",
    "Each symbol (except $<beg>$) in the sequence is predicted using the same conditional probability table\n",
    "until an $<end>$ symbol is seen\n",
    "\n",
    "<img src=\"img/12.23.png\" width=\"400\"/>\n",
    "\n",
    "<b>Maximum likelihood estimation: </b> The goal is to maximize the probability that the model can generate all the observed sentences (corpus S)\n",
    "$$s \\in S, \\quad s=\\left \\{ w_1^s,w_2^s, ... , w_{\\left | s \\right |}^s \\right \\}$$\n",
    "\n",
    "The ML estimate is obtained as normalized counts of successive word occurrences (matching statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example1 \n",
    "The probability of generating the following sentence <span style=\"color:blue\">$<beg>$ ML course UNK $<end>$</span> is\n",
    "\n",
    "ùëÉ(ùëÄùêø|<ùëèùëíùëî>)√óùëÉ(ùëêùëúùë¢ùëüùë†ùëí|ùëÄùêø)√óùëÉ(ùëàùëÅùêæ|ùëêùëúùë¢ùëüùë†ùëí)√óùëÉ(<ùëíùëõùëë>|ùëàùëÅùêæ) = 0.7√ó0.5√ó0.1√ó0.2 = 0.007 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example2\n",
    "Some sentences below CANNOT be generated.\n",
    "- <span style=\"color:blue\">$<beg>$ course ML is UNK $<end>$</span>\n",
    "- <span style=\"color:blue\">$<beg>$  $<end>$</span>\n",
    "- <span style=\"color:blue\">course is ML $<end>$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example3\n",
    "Suppose our training examples are the following three sentences.\n",
    "- ML courses are cool.\n",
    "- Humanities courses are cool.\n",
    "- But some courses are boring.\n",
    "\n",
    "Using a BIGRAM model, the maximum likelihood estimate for the probability that the next word is 'cool', given that the previous word is 'are', is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<beg>', 'Humanities'): 1,\n",
       " ('Humanities', 'courses'): 1,\n",
       " ('courses', 'are'): 3,\n",
       " ('are', 'cool'): 2,\n",
       " ('cool', '<end>'): 2,\n",
       " ('<beg>', 'But'): 1,\n",
       " ('But', 'some'): 1,\n",
       " ('some', 'courses'): 1,\n",
       " ('are', 'boring'): 1,\n",
       " ('boring', '<end>'): 1,\n",
       " ('<beg>', 'ML'): 1,\n",
       " ('ML', 'courses'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Humanities courses are cool.', 'But some courses are boring.', 'ML courses are cool.']\n",
    "data = [['<beg>']+i.replace('.',' <end>').split(' ') for i in data]\n",
    "data = [bi_w for this_s in data for bi_w in zip(this_s[:-1],this_s[1:])]\n",
    "counter=collections.Counter(data)\n",
    "bow = dict(counter)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[('are', 'cool')]/sum([v for i,v in bow.items() if i[0] == 'are'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature based Markov Model\n",
    "We can also represent the Markov model as a feed-forward neural network (very extendable)\n",
    "Take softmax activation of our outputs. Say given a word i, let the probability that word j occurs next to i be  $p_j$ . $p_j$ satisfies:\n",
    "\n",
    "$$\\sum_{k =1}^{K}p_k=1 \\quad \\text{where } p_k \\geqslant 0 \\text{ for } \\forall k \\in K$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the words are one-hot encoded, so each input word would activate one unique node on the input layer.\n",
    "\n",
    "Advantages of the FFNNs vs Markov models are:\n",
    "- contain a fewer number of parameters\n",
    "\n",
    "A Markov model would have 100 choices for the previous two words, and 10 choices for the next word, leading to a size of 1000. A feedforward neural network would have an input layer of size 20 and an output layer of size 10, leading to a weight matrix of size 200. We add 10 parameters for the bias vector.\n",
    "\n",
    "- can easily control the complexity of a FFNN by introducing hidden layers\n",
    "\n",
    "However,any information encoded in a neural network could also be encoded in a very large transition probability matrix, i.e. a Markov Model. Therefore, the essential information is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal / sequence problems\n",
    "Language modeling: what comes next?\n",
    "\n",
    "<img src=\"img/12.23.png\" width=\"300\"/>\n",
    "\n",
    "A trigram language model\n",
    "\n",
    "<img src=\"img/9.4.png\" width=\"300\"/>\n",
    "<img src=\"img/9.4.png\" width=\"300\"/>\n",
    "<img src=\"img/9.4.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some real cases\n",
    "<img src=\"img/12.29.png\" width=\"500\"/><br>\n",
    "<img src=\"img/12.30.png\" width=\"800\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Markov models for sequences\n",
    "- how to formulate, estimate, sample sequences from\n",
    "\n",
    "RNNs for generating (decoding) sequences - relation to Markov models\n",
    "- evolving hidden state\n",
    "- sampling from\n",
    "\n",
    "Decoding vectors into sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Converlutional Neural Networks (CNNs)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Case\n",
    "$$(f*g)(t)\\equiv\\int_{-\\infty}^{+\\infty}f(\\tau)g(t-\\tau)d\\tau$$\n",
    "ùúè  is the dummy variable for integration and  ùë°  is the parameter. Intuitively, convolution 'blends' the two function  ùëì  and  ùëî  by expressing the amount of overlap of one function as it is shifted over another function.\n",
    "\n",
    "<img src=\"img/images_L12_cov_f.png\" width=\"300\"/><img src=\"img/images_L12_cov_g.png\" width=\"300\"/><img src=\"img/images_L12_cov_f*g.png\" width=\"300\"/>\n",
    "\n",
    "The area under the convolution: $\\int_{-\\infty}^{+\\infty}(f*g)dt$ is the product of the areas under  ùëì  and  ùëî<br>\n",
    "<img src=\"img/12.1.png\" width=\"400\"/>\n",
    "\n",
    "#### Discrete Case\n",
    "$$(f*g)[n]\\equiv\\sum_{m=-\\infty}^{m=+\\infty}f[m]g[n-m]$$\n",
    "\n",
    "1. 1D discrete signal example:\n",
    "\n",
    "Let  ùëì[ùëõ]=[1,2,3] ,  ùëî[ùëõ]=[2,1]  and suppose  ùëõ  starts from  0 . We are computing  ‚Ñé[ùëõ]=ùëì[ùëõ]‚àóùëî[ùëõ] .\n",
    "As  ùëì  and  ùëî  are finite signals, we just put  0  to where  ùëì  and  ùëî  are not defined. This is usually called zero padding. Now, let's compute  ‚Ñé[ùëõ] step by step:<br>\n",
    "<img src=\"img/12.2.png\" width=\"800\"/><br>\n",
    "The other parts of  ‚Ñé  are all  0 .\n",
    "\n",
    "<img src=\"img/images_L12_cov_example.png\" width=\"600\"/>\n",
    "\n",
    "In practice, it is common to call the flipped  ùëî‚Ä≤  as filter or kernel, for the input signal or image  ùëì .\n",
    "\n",
    "As we forced to pad zeros to where the input are not defined, the result on the edge of the input may not be accurate. To avoid this, we can just keep the convolution result where the input  ùëì  is actually defined. That is  ‚Ñé[ùëõ]=[5,8] .\n",
    "\n",
    "So with zero padding, ‚Ñé[ùëõ]=[2,5,8,3];  without zero padding, ‚Ñé[ùëõ]=[5,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 2D discrete signal example:\n",
    "\n",
    "$$f=\\begin{bmatrix}1 & 2 & 1\\\\ 2 & 1 & 1 \\\\1&1&1\\end{bmatrix} \\qquad g'=\\begin{bmatrix}1 & 0.5\\\\ 0.5 & 1\\end{bmatrix} \\qquad$$\n",
    "Without zero padding, we have $h=\\begin{bmatrix}4 & 4\\\\ 4 & 3\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "Pooling region and ‚Äústride‚Äù may vary\n",
    "- pooling induces translation invariance at the cost of spatial resolution\n",
    "- stride reduces the size of the resulting feature map\n",
    "\n",
    "<img src=\"img/12.3.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "convert the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer.\n",
    "\n",
    "<img src=\"img/12.20.png\" width=\"300\"/>\n",
    "<img src=\"img/12.21.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Example\n",
    "Say we have just one conv layer consisting of just one filter  ùêπ  of shape  2√ó2 followed by a max-pooling layer of shape  2√ó2 . The input image is of shape  3√ó3.\n",
    "\n",
    "Assuming that the stride for the convolution and pool layers is  1. And our image I and filter weights F are below.\n",
    "$$I=\\begin{bmatrix}1 & 0 & 2\\\\ 3 & 1 & 0 \\\\ 0&0&4\\end{bmatrix} \\qquad F=\\begin{bmatrix}1 & 0\\\\ 0 & 1\\end{bmatrix} \\qquad$$\n",
    "\n",
    "The output of the CNN is calculated as $Pool(ReLU(Conv(I)))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the final output is 5\n"
     ]
    }
   ],
   "source": [
    "I = np.array([[1,0,2],[3,1,0],[0,0,4]])\n",
    "F = np.array([[1,0],[0,1]])\n",
    "# without zero padding\n",
    "Conv_I = convolve(I,F)[:-1,:-1]\n",
    "ReLU_I = np.maximum(0, Conv_I)\n",
    "Pool_I = np.max(ReLU_I)\n",
    "print('Thus, the final output is '+str(Pool_I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "Documentation: https://pytorch.org/docs/stable/nn.html#conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some real cases\n",
    "<img src=\"img/12.4.png\" width=\"500\"/><br>(LeCun 13‚Äô)\n",
    "\n",
    "<img src=\"img/12.5.png\" width=\"500\"/><br>(Krizhevsky et al., 12‚Äô)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Optimization Algorithms</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange;\">Gradient Descent</span>\n",
    "$$\\theta=\\theta-\\eta\\cdot \\nabla J(\\theta)$$\n",
    "\n",
    "<img src=\"img/12.6.png\" width=\"350\">\n",
    "- Cons: if the Weight(W) values are too small or too large then we have large Errors , so want to update and optimize the weights such that it is neither too small nor too large , so we descent downwards opposite to the Gradients until we find a local minima.\n",
    "\n",
    "### Variants of Gradient Descent\n",
    "##### <span style=\"color:orange;\">1. Stochastic gradient descent</span>\n",
    "$$\\theta=\\theta-\\eta\\cdot \\nabla J(\\theta;x^{(i)};y^{(i)})$$\n",
    "- Pros:usually much faster technique. Also, due to these frequent updates ,parameters updates have high variance and causes the Loss function to fluctuate to different intensities. This helps us discover new and possibly better local minima , whereas Standard Gradient Descent will only converge to the minimum of the basin as mentioned above.\n",
    "- Cons:due to the frequent updates and fluctuations it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations. Although, it has been shown that as we slowly decrease the learning rate-Œ∑, SGD shows the same convergence pattern as Standard gradient descent.\n",
    "\n",
    "<img src=\"img/12.7.png\" width=\"350\">\n",
    "\n",
    "##### <span style=\"color:orange;\">2. Mini Batch Gradient Descent</span>\n",
    "- Pros: very efficient; rectify the problems of high variance parameter updates and unstable convergence discussed above and thus lead to a much better and stable convergence; Commonly Mini-batch sizes Range from 50 to 256, but can vary as per the application and problem being solved.\n",
    "- Cons: still difficult to choose a proper learning rate; cannot apply a different learning rate to different para updates; can get trapped in numerous sub-optimal local minima\n",
    "\n",
    "##### <span style=\"color:orange;\">3. Mini Batch Stochastic Gradient Descent</span>\n",
    "\n",
    "### Optimizing the Gradient Descent\n",
    "##### <span style=\"color:orange;\">1. Momentum</span>\n",
    "$$V_t=\\gamma V_{t-1}+\\eta\\cdot \\nabla J(\\theta) \\quad\\rightarrow \\quad \\theta=\\theta-V_t $$\n",
    " \n",
    "The momentum term Œ≥ is usually set to 0.9 or a similar value. It's a fraction of the update vector of the past step to the current update vector.\n",
    "\n",
    "Here the momentum is same as the momentum in classical physics , as we throw a ball down a hill it gathers momentum and its velocity keeps on increasing.It does parameter updates only for relevant examples.\n",
    "\n",
    "- Pros: accelerates SGD by navigating along the relevant direction and softens the high variance oscillations in irrelevant directions; leads to faster and stable convergence.\n",
    "- Cons: A ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We‚Äôd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:orange;\">2. Nesterov accelerated gradient</span>\n",
    "$$V_t=\\gamma V_{t-1}+\\eta\\cdot \\nabla J(\\theta-\\gamma V_{t-1}) \\quad\\rightarrow \\quad \\theta=\\theta-V_t $$\n",
    "\n",
    "1. make a big jump based on out previous momentum \n",
    "2. calculate the Gradient \n",
    "3. make an correction which results in an parameter update. \n",
    "\n",
    "- Pros: This anticipatory update prevents us to go too fast and not to miss the minima and makes it more responsive to changes.\n",
    "- Cons: we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.\n",
    "\n",
    "##### <span style=\"color:orange;\">3. Ada-grad</span>\n",
    "$$\\theta_{t+1,i}=\\theta_{t,i}-\\frac{\\eta}{\\sqrt{G_{t,ii}+\\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "Most implementations use a default value of 0.01 and leave it at that.\n",
    "\n",
    "- Pros: need to manually tune the learning Rate Œ∑. It makes big updates for infrequent parameters and small updates for frequent parameters. Thus, it is well-suited for dealing with sparse data.\n",
    "- Cons: its learning rate Œ∑ is always decaying and becoming so small due to the accumulation of each squared Gradients in the denominator , since every added term is positive. As a result, the model has very slow convergence and eventually just stops learning entirely and stops acquiring new additional knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:orange;\">4. AdaDelta</span>\n",
    "$$E[g^2]_t=\\gamma E[g^2]_{t-1}+(1-\\gamma)\\cdot g^2(t)$$\n",
    "\n",
    "Instead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying mean of all past squared gradients. \n",
    "\n",
    "The running average $E[g^2]_t$ at time step t then depends (as a fraction Œ≥ similarly to the Momentum term) only on the previous average and the current gradient.\n",
    "\n",
    "set Œ≥ to a similar value as the momentum term, around 0.9.\n",
    "\n",
    "$$\\Delta\\theta_t=-\\eta\\cdot g_{t,i}\\quad \\rightarrow \\quad \\theta_{t+1}=\\theta_t+\\Delta\\theta_t$$\n",
    "$$\\Delta \\theta_{t}=-\\frac{\\eta}{\\sqrt{E\\left | g^2 \\right |_t+\\varepsilon}} \\cdot g_{t}$$\n",
    "\n",
    "When the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:\n",
    "\n",
    "$$\\Delta \\theta_{t}=-\\frac{\\eta}{RMS\\left | g \\right |_t} \\cdot g_{t}$$\n",
    "\n",
    "- Pros: rectify the problem of Decaying Learning Rate. Instead of accumulating all previous squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w.Don‚Äôt even need to set a default learning Rate .\n",
    "\n",
    "What we achieved so far:\n",
    "1. calculate different learning Rates for each parameter.\n",
    "2. calculate momentum.\n",
    "3. prevent Vanishing(decaying) learning Rates.\n",
    "\n",
    "##### <span style=\"color:orange;\">3. Adaptive Moment Estimation (Adam)</span>\n",
    "$$\\hat{m_t}=\\frac{m_t}{1-\\beta_1^t} \\qquad \\hat{v_t}=\\frac{v_t}{1-\\beta_2^t} $$\n",
    "\n",
    "keeps an exponentially decaying average of past gradients $M_t$, similar to momentum. Here, $M_t$ and $V_t$ are values of the first moment which is the Mean and the second moment which is the uncentered variance of the gradients respectively.\n",
    "\n",
    "$$\\theta_{t+1}=\\theta_{t}-\\frac{\\eta}{\\sqrt{\\hat{v_t}+\\varepsilon}} \\cdot \\hat{m_t}$$\n",
    "\n",
    "The values for Œ≤1 is 0.9 , 0.999 for Œ≤2, and (10 x exp(-8)) for œµ.\n",
    "\n",
    "- Pros: not only computes adaptive learning rates for each parameter, but also calculate individual momentum changes for each parameter and store them separately.\n",
    "Adam works well in practice and compares favorably to other adaptive learning-method algorithms as it converges very fast and the learning speed of the Model is quiet Fast and efficient and also it rectifies every problem that is faced in other optimization techniques such as vanishing Learning rate , slow convergence or High variance in the parameter updates which leads to fluctuating Loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "Adam or any other Adaptive learning rate techniques outperform every other optimization algorithms.\n",
    "\n",
    "<img src=\"img/12.8.png\" width=\"500\">\n",
    "<img src=\"img/12.9.gif\" width=\"500\">\n",
    "<img src=\"img/12.10.gif\" width=\"500\">\n",
    "<img src=\"img/12.11.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <span style=\"color:green\">Regularization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function = Loss (say, binary cross entropy) + Regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange;\">L1 & L2 Regularization</span>\n",
    "1. L2: $$Cost function = Loss + \\frac{\\lambda}{2m}\\cdot\\sum\\left \\| w \\right \\|^2$$\n",
    "\n",
    "L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).\n",
    "\n",
    "2. L1: $$Cost function = Loss + \\frac{\\lambda}{2m}\\cdot\\sum\\left \\| w \\right \\|$$\n",
    "\n",
    "Unlike L2, the weights may be reduced to zero here. Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange;\">Dropout</span>\n",
    "<img src=\"img/12.12.png\" width=\"250\">\n",
    "<img src=\"img/12.13.png\" width=\"250\">\n",
    "\n",
    "So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.\n",
    "\n",
    "<img src=\"img/12.14.gif\" width=\"500\">\n",
    "\n",
    "It produces very good results and is consequently the most frequently used regularization technique in the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange;\">Data Augmentation</span>\n",
    "The simplest way to reduce overfitting is to increase the size of the training data. In machine learning, we were not able to increase the size of training data as the labeled data was too costly.\n",
    "\n",
    "There are a few ways of increasing the size of the training data ‚Äì rotating the image, flipping, scaling, shifting, etc. \n",
    "\n",
    "<img src=\"img/12.15.png\" width=\"500\">\n",
    "<img src=\"img/12.16.png\" width=\"500\">\n",
    "<img src=\"img/12.17.png\" width=\"500\">\n",
    "<img src=\"img/12.18.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange;\">Early Stopping</span>\n",
    "A kind of cross-validation strategy where we keep one part of the training set as the validation set. When we see that the performance on the validation set is getting worse, we immediately stop the training on the model. This is known as early stopping.\n",
    "\n",
    "There are a few ways of increasing the size of the training data ‚Äì rotating the image, flipping, scaling, shifting, etc. \n",
    "\n",
    "<img src=\"img/12.19.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU to run models\n",
    "Google Colab (free):\n",
    "https://colab.research.google.com/notebooks/intro.ipynb#recent=true\n",
    "\n",
    "Paperspace (<$1/hr):\n",
    "https://www.paperspace.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet, AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
